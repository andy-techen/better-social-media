{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e40eff4-53ea-4148-a559-a98c56c66058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATD+Rand_training.csv\tDATD_and_DATD+Rand_test.csv  DATD_training.csv\n"
     ]
    }
   ],
   "source": [
    "!ls DATD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26cb5945-29c2-4a01-85dc-dfda0ae0c8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "     |████████████████████████████████| 24.1 MB 2.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /home/ccmilne/.local/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/ccmilne/.local/lib/python3.8/site-packages (from gensim) (1.21.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ccmilne/.local/lib/python3.8/site-packages (from gensim) (1.7.1)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.1.2\n",
      "\u001b[33mWARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d3b64f8-5bef-4bf9-877b-8808090a45b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "     |████████████████████████████████| 1.5 MB 1.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/ccmilne/.local/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ccmilne/.local/lib/python3.8/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ccmilne/.local/lib/python3.8/site-packages (from nltk) (2021.9.30)\n",
      "Requirement already satisfied: tqdm in /home/ccmilne/.local/lib/python3.8/site-packages (from nltk) (4.62.3)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.6.5\n",
      "\u001b[33mWARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08f86aa0-2eeb-4f09-a7be-b8abd758c249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ccmilne/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ccmilne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47c82c83-3fe3-4bfc-9499-a9f057ae9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_train_data = pd.read_csv(\"datd/DATD+Rand_training.csv\")\n",
    "train_data = pd.read_csv(\"datd/DATD_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a45c860e-2fba-44ac-8ddf-57b51dad493f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               @user Me talking to my antidepressants\n",
       "1    going to attempt at least to change this sour ...\n",
       "2            flighty sag placements make me anxioussss\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45df35b3-28b6-4386-93d6-0dbeaaf98c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MENTAL_HEALTH    473\n",
       "OTHER            427\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94587258-a9ad-4d5b-a82d-3834ab38d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(tweets_array):\n",
    "    \n",
    "    \"\"\"\n",
    "    Take in an array of tweets, and return the processed tweets.\n",
    "    \n",
    "    (e.g. input: 'i am depressed', output - 'am depressed')  -> since I remove those words with length 1\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    processed_array = []\n",
    "    \n",
    "    for tweet in tqdm(tweets_array):\n",
    "        \n",
    "        # remove other non-alphabets symbols with space (i.e. keep only alphabets and whitespaces).\n",
    "        processed = re.sub('[^@a-zA-Z ]', '', tweet)\n",
    "\n",
    "        #Usernames (anything that begins with \"@\")\n",
    "        no_usernames = \" \".join([x for x in processed.split() if x[0] != '@'])\n",
    "        \n",
    "        #words = processed.split()\n",
    "        words = word_tokenize(no_usernames)\n",
    "\n",
    "        #Stopwords\n",
    "        tokens_without_sw = [word for word in words if not word in stopwords.words()]\n",
    "        \n",
    "        # keep words that have length of more than 1 (e.g. gb, bb), remove those with length 1.\n",
    "        processed_array.append(' '.join([word for word in tokens_without_sw if len(word) > 1]))\n",
    "    \n",
    "    return processed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f072a5c-b9ce-4835-ae70-c71120aae4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 900/900 [01:25<00:00, 10.51it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data['processed'] = preprocessing(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75bd470c-2f78-43a0-bd7f-b11be24db55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user Me talking to my antidepressants</td>\n",
       "      <td>MENTAL_HEALTH</td>\n",
       "      <td>Me talking antidepressants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>going to attempt at least to change this sour ...</td>\n",
       "      <td>MENTAL_HEALTH</td>\n",
       "      <td>going attempt least change sour mood Its mainl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flighty sag placements make me anxioussss</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>flighty sag placements make anxioussss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm going to buy cream cheese after work and s...</td>\n",
       "      <td>MENTAL_HEALTH</td>\n",
       "      <td>Im going buy cream cheese work bathtub eat Thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My anxiety and stress levels for today are act...</td>\n",
       "      <td>MENTAL_HEALTH</td>\n",
       "      <td>My anxiety stress levels today actually making...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Libra culture is using the same nail polish co...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>Libra culture using nail polish color salon vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>anyways, my last year of college starts today ...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>anyways last year college starts today whew an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>'Dad.. I'm depressed.' 'Hi depressed, I'm Dad.'</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>Dad Im depressed Hi depressed Im Dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>respiratory depression be my god</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>respiratory depression god</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>deactivated my main because not only do i feel...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>deactivated main feel invisible depressed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text          label  \\\n",
       "0               @user Me talking to my antidepressants  MENTAL_HEALTH   \n",
       "1    going to attempt at least to change this sour ...  MENTAL_HEALTH   \n",
       "2            flighty sag placements make me anxioussss          OTHER   \n",
       "3    I'm going to buy cream cheese after work and s...  MENTAL_HEALTH   \n",
       "4    My anxiety and stress levels for today are act...  MENTAL_HEALTH   \n",
       "..                                                 ...            ...   \n",
       "895  Libra culture is using the same nail polish co...          OTHER   \n",
       "896  anyways, my last year of college starts today ...          OTHER   \n",
       "897    'Dad.. I'm depressed.' 'Hi depressed, I'm Dad.'          OTHER   \n",
       "898                   respiratory depression be my god          OTHER   \n",
       "899  deactivated my main because not only do i feel...          OTHER   \n",
       "\n",
       "                                             processed  \n",
       "0                           Me talking antidepressants  \n",
       "1    going attempt least change sour mood Its mainl...  \n",
       "2               flighty sag placements make anxioussss  \n",
       "3    Im going buy cream cheese work bathtub eat Thi...  \n",
       "4    My anxiety stress levels today actually making...  \n",
       "..                                                 ...  \n",
       "895  Libra culture using nail polish color salon vi...  \n",
       "896  anyways last year college starts today whew an...  \n",
       "897               Dad Im depressed Hi depressed Im Dad  \n",
       "898                         respiratory depression god  \n",
       "899          deactivated main feel invisible depressed  \n",
       "\n",
       "[900 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b6192ba-52b9-4fdc-b33f-d284e324beda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MENTAL_HEALTH': 0, 'OTHER': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.asarray(train_data['processed'])\n",
    "y = np.asarray(train_data['label'])\n",
    "\n",
    "label_map = {cat: index for index, cat in enumerate(np.unique(y))}\n",
    "y_prep = np.asarray([label_map[l] for l in y])\n",
    "\n",
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897468a-61c7-4d19-9842-5c868c67ac20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8053bcd9-ba0d-4502-b502-0331f062ac8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Me', 'talking', 'antidepressants']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokenized = [[w for w in sentence.split(\" \") if w != \"\"] for sentence in x]\n",
    "x_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12bd0582-7efb-4952-9200-95eab2a46b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process took 0.07 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = Word2Vec(\n",
    "            x_tokenized,\n",
    "            vector_size=900 # Size is the length of our vector.\n",
    "        )\n",
    "\n",
    "end = round(time.time()-start, 2)\n",
    "print(\"This process took\",end,\"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8763668-429b-49d9-992d-5e7e6e873300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anxiety', 0.6839107871055603),\n",
       " ('depressed', 0.6795772910118103),\n",
       " ('Im', 0.6524881720542908),\n",
       " ('feel', 0.6221291422843933),\n",
       " ('dont', 0.6006389856338501),\n",
       " ('anxious', 0.5870542526245117),\n",
       " ('know', 0.5678699016571045),\n",
       " ('people', 0.5413738489151001),\n",
       " ('get', 0.5381070375442505),\n",
       " ('feeling', 0.5354692935943604)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"depression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "941d3834-5193-448e-ba65-cfb0560e0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequencer():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 all_words,\n",
    "                 max_words,\n",
    "                 seq_len,\n",
    "                 embedding_matrix\n",
    "                ):\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.embed_matrix = embedding_matrix\n",
    "        \"\"\"\n",
    "        temp_vocab = Vocab which has all the unique words\n",
    "        self.vocab = Our last vocab which has only most used N words.\n",
    "    \n",
    "        \"\"\"\n",
    "        temp_vocab = list(set(all_words))\n",
    "        self.vocab = []\n",
    "        self.word_cnts = {}\n",
    "        \"\"\"\n",
    "        Now we'll create a hash map (dict) which includes words and their occurencies\n",
    "        \"\"\"\n",
    "        for word in temp_vocab:\n",
    "            # 0 does not have a meaning, you can add the word to the list\n",
    "            # or something different.\n",
    "            count = len([0 for w in all_words if w == word])\n",
    "            self.word_cnts[word] = count\n",
    "            counts = list(self.word_cnts.values())\n",
    "            indexes = list(range(len(counts)))\n",
    "        \n",
    "        # Now we'll sort counts and while sorting them also will sort indexes.\n",
    "        # We'll use those indexes to find most used N word.\n",
    "        cnt = 0\n",
    "        while cnt + 1 != len(counts):\n",
    "            cnt = 0\n",
    "            for i in range(len(counts)-1):\n",
    "                if counts[i] < counts[i+1]:\n",
    "                    counts[i+1],counts[i] = counts[i],counts[i+1]\n",
    "                    indexes[i],indexes[i+1] = indexes[i+1],indexes[i]\n",
    "                else:\n",
    "                    cnt += 1\n",
    "        \n",
    "        for ind in indexes[:max_words]:\n",
    "            self.vocab.append(temp_vocab[ind])\n",
    "                    \n",
    "    def textToVector(self,text):\n",
    "        # First we need to split the text into its tokens and learn the length\n",
    "        # If length is shorter than the max len we'll add some spaces (100D vectors which has only zero values)\n",
    "        # If it's longer than the max len we'll trim from the end.\n",
    "        tokens = text.split()\n",
    "        len_v = len(tokens)-1 if len(tokens) < self.seq_len else self.seq_len-1\n",
    "        vec = []\n",
    "        for tok in tokens[:len_v]:\n",
    "            try:\n",
    "                vec.append(self.embed_matrix[tok])\n",
    "            except Exception as E:\n",
    "                pass\n",
    "        \n",
    "        last_pieces = self.seq_len - len(vec)\n",
    "        for i in range(last_pieces):\n",
    "            vec.append(np.zeros(100,))\n",
    "        \n",
    "        return np.asarray(vec).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a610831-ec16-4bbd-bab7-2645c3fefa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [token for seq in x_tokenized for token in seq]\n",
    "max_words = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69dbe773-0778-4ca1-98b4-653316802bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencer = Sequencer(\n",
    "                all_words = all_words,\n",
    "                max_words = max_words,\n",
    "                seq_len = 15,\n",
    "                embedding_matrix = model.wv\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ceb61ad-9f64-47ba-9060-a87af89218f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_172/2991566974.py:63: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(vec).flatten()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00011371,  0.00107021,  0.0002885 ,  0.00085611,  0.00125629,\n",
       "       -0.00129817,  0.00017513,  0.00118605,  0.00035596,  0.00015801],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vec = sequencer.textToVector(\"i am in love with you\")\n",
    "test_vec[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06f27f22-c5a7-49ec-a31a-495e32d7bdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f2881-4712-49c5-9567-0dd62c4eac2c",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4bce53f-49da-4fc4-a0e2-f2308d22aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_172/2991566974.py:63: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(vec).flatten()\n",
      "/tmp/ipykernel_172/524339332.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_vecs = np.asarray([sequencer.textToVector(\" \".join(seq)) for seq in x_tokenized])\n"
     ]
    }
   ],
   "source": [
    "x_vecs = np.asarray([sequencer.textToVector(\" \".join(seq)) for seq in x_tokenized])\n",
    "print(x_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28502976-c784-481e-a3fd-71332f960430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720,)\n",
      "(180,)\n",
      "(720,)\n",
      "(180,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_vecs, y_prep, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2af445db-6f87-4300-b581-42d0d4584f3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_172/2522205720.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msvm_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msvm_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    191\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    957\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    736\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m                 raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "process = round(end-start,2)\n",
    "print(\"Support Vector Machine Classifier has fitted, this process took {} seconds\".format(process))\n",
    "svm_classifier.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
